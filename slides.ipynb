{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"images/criteoAiLab.png\" heigth=45>\n",
    "\n",
    "# Practical Session : Deep RL\n",
    "\n",
    "## Agenda\n",
    "\n",
    "  1. Deep-RL Theory Reminder (15')\n",
    "  1. DQN (30')\n",
    "  1. $\\epsilon$-DQN (20')\n",
    "  1. DQN with Replay (20')\n",
    "  1. Potential problems & solutions (10')\n",
    "  1. Wrap-up (5')\n",
    "  \n",
    "(!) Regular polls/quiz\n",
    "  \n",
    "  <img src=\"images/logo.png\" width=\"250px\" align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep-RL Theory\n",
    "   \n",
    "<img src=\"images/rl.png\" width=\"480\" alt=\"test\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Goal**: Maximise w.r.t $\\pi$ to collect more cumulated discounted reward \n",
    "  - $R = \\sum \\limits_{t=0}^{\\infty} \\gamma^t r_t$\n",
    "  - $0<\\gamma<1$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reinforcement Learning for a Markov Decision Process \n",
    "\n",
    "Under a fixed policy, $V$ is the value of the state:\n",
    "\n",
    "<img src=\"images/vpi.png\" width=\"480\"/>\n",
    "\n",
    "Under a fixed policy, $Q$ is the value of the tuple (state, action):\n",
    "\n",
    "<img src=\"images/qpi.png\" width=\"640\"/>\n",
    "\n",
    "Remark: after time $t$, actions are chosen by $\\pi$ so in expectation we could replace $Q^\\pi(s_{t+1}, a_{t+1})$ by $V^\\pi(s_{t+1})$\n",
    "\n",
    "### Q-learning\n",
    "\n",
    " - Idea is to learn $Q$, hence to know which action is best in a given state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bellman Optimality Principle\n",
    "\n",
    "The optimal policy $\\pi^*$ must respect \n",
    "\n",
    "<img src=\"images/bellman.png\" width=320>\n",
    "\n",
    "**Strategy**: While this equality doesn't hold we'll try to improve $\\pi$\n",
    "\n",
    "For a fixed policy $\\pi$ if the quantity are not equals :\n",
    "  - either $Q$ is not correctly estimated (policy evaluation issue) \n",
    "  - or $\\pi$ is not selecting optimal actions, this can be fixed being more greedy wrt $Q$\n",
    "\n",
    "<img src=\"images/evalimprov.png\" width=480>\n",
    "\n",
    "<center><sup>(R. S. Sutton, 1998, Reinforcement Learning An introduction)</sup></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Q-Learning (DQN)\n",
    "\n",
    "<img src=\"images/dqn-th.png\" width=640>\n",
    "\n",
    "**Q-learning**:\n",
    "\n",
    "While $Q^\\pi(s_t, a_t) \\ne r_t + \\gamma \\ max \\ {Q^\\pi(s_{t+1}, a)}$:\n",
    "  1. take action\n",
    "  1. receive reward / new state \n",
    "  1. improve target evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Deep Q-Learning (DQN)\n",
    "  - $Q$ modeled as a NN\n",
    "  \n",
    "  - inputs = states\n",
    "  - outputs = actions\n",
    "\n",
    "<center><img src=\"images/q-learning.png\" width=480><center>\n",
    "<center><sup><sub>Image from https://leonardoaraujosantos.gitbooks.io</sub></sup></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep-RL Benchmarks\n",
    "\n",
    "## OpenAI \n",
    "\n",
    "  - [Gym](https://gym.openai.com/envs/CartPole-v0/) : set of standard problems / environments\n",
    "\n",
    "### Simple control problems\n",
    "\n",
    "<img src=\"images/control.png\" width=480>\n",
    "\n",
    "### Famous learn to play Atari\n",
    "\n",
    "<img src=\"images/atari.png\" width=480>\n",
    "<center><sup><sub>Images from https://github.com/dgriff777/rl_a3c_pytorch</sub></sup></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class RLEnvironment:\n",
    "\n",
    "    def run(self, agent, episodes=100, ...):\n",
    "        \"\"\"\n",
    "        Run the agent.\n",
    "\n",
    "        Pseudo-code:\n",
    "        ```\n",
    "            for i in 1..episodes {\n",
    "\n",
    "                start new episode\n",
    "\n",
    "                while episode not finished {\n",
    "\n",
    "                    ask agent to take action based on current state\n",
    "                    action resolved by environment, returning reward and new state\n",
    "\n",
    "                    <opportunity to feedback agent with (state, reward, new state)>\n",
    "                    <opportunity to update agent parameters>\n",
    "\n",
    "                    if new state means agent failed {\n",
    "                        terminate episode\n",
    "                    }\n",
    "\n",
    "                }\n",
    "\n",
    "                <opportunity to update agent parameters again>\n",
    "\n",
    "                if last episodes show enough reward {\n",
    "                    declare task solved\n",
    "                }\n",
    "\n",
    "            }\n",
    "\n",
    "        ```\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    \"\"\"The world's simplest agent!\"\"\"\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def get_action(self, state):\n",
    "        return self.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "env.run(RandomAgent(env.action_space), episodes=20, display_policy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### CartPole\n",
    "  - $(x, \\dot x)$: position/speed of cart\n",
    "  - $(\\theta, \\dot \\theta)$: angle/angular velocity of the pole\n",
    "  - actions: move left/right\n",
    "  - environment simulates acceleration (cart/pole mass)\n",
    "<img src=\"images/cartpole.gif\" width=640>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. DQN in practice\n",
    "  - Skeleton provided, implemented with [Keras](https://keras.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class DQNAgent(RLDebugger):\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        self.learning_rate = ??? \n",
    "        \n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "   \n",
    "        model.add(Dense(???, input_dim=self.state_size, activation=???))\n",
    "        model.add(Dense(self.action_size, activation=???))\n",
    "\n",
    "        model.compile(loss=???, optimizer=Adam(lr=self.learning_rate))\n",
    "\n",
    "        model.summary()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "    # get action from model using greedy policy. \n",
    "    def get_action(self, state):\n",
    "        q_value = self.model.predict(state)\n",
    "        best_action = np.argmax(q_value[0])\n",
    "        return best_action\n",
    "\n",
    "    # train the target network on the selected action and transition\n",
    "    def train_model(self, action, state, next_state, reward, done):\n",
    "        target = self.model.predict(state)\n",
    "\n",
    "        target_val = self.target_model.predict(next_state)\n",
    "\n",
    "        if done:\n",
    "            target[0][action] = reward\n",
    "        else:\n",
    "            target[0][action] = reward + self.gamma * (np.amax(target_val))\n",
    "\n",
    "        loss = self.model.fit(state, target, verbose=0).history['loss'][0]\n",
    "        self.record(action, state, target, target_val, loss)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "agent = DQNAgent(env.observation_space, env.action_space)\n",
    "env.run(agent, episodes=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_1 (Dense)              (None, 30)                150       \n",
    "=================================================================\n",
    "Total params: 150\n",
    "Trainable params: 150\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "Episode 10, Total reward 9.0\n",
    "Episode 20, Total reward 9.0\n",
    "Episode 30, Total reward 10.0\n",
    "Episode 40, Total reward 10.0\n",
    "Episode 50, Total reward 10.0\n",
    "Episode 60, Total reward 10.0\n",
    "Episode 70, Total reward 10.0\n",
    "Episode 80, Total reward 10.0\n",
    "Episode 90, Total reward 10.0\n",
    "Episode 100, Total reward 10.0\n",
    "Average Total Reward of last 100 episodes: 9.94\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Evaluation\n",
    "- **Objective:** average reward of 200 over 100 episodes\n",
    "- *First Target:* reach a reward > 50 (at least once)\n",
    "- We only focus on tuning model parameters for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "___\n",
    "\n",
    "**Your turn now !**\n",
    "\n",
    "```\n",
    "$ jupyter notebook exercises.ipynb\n",
    "```\n",
    "\n",
    "Be warned: your results will vary from one run to the other (random init of the model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DQN First Results\n",
    "\n",
    "### POLL: who reached a reward > 50?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### POLL: how many neurons? more than 20, 50, 100?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### POLL: how many layers? 1,2, more?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### POLL: what loss? mse, mean_absolute_error, another?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### POLL: what learning rate? more than $10^{-4}, 10^{-3}, 10^{-2}?$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Case Study\n",
    "\n",
    "env.run(agent, episodes=100, seed=0)\n",
    "\n",
    "### Symptom\n",
    "\n",
    "```\n",
    "agent.plot_loss()\n",
    "```\n",
    "\n",
    "| | |\n",
    "|--|--|\n",
    "| ![](images/loss_dqn_1.png) | ```Total Reward: 10.0``` |\n",
    "\n",
    "### POLL: Is it a problem of ...\n",
    "  - Model capacity (#neurons) ?\n",
    "  - Loss function ?\n",
    "  - Activation ?\n",
    "  - Exploration ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```\n",
    "agent.plot_action()\n",
    "```\n",
    "\n",
    "![](images/action_dqn_1.png)\n",
    "\n",
    "**Take away:** Q-learning can converge only if it explores enough actions (hence states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DQN with Exploration\n",
    "\n",
    "Exploration of the states is crucial for performance\n",
    "\n",
    "  - add an uniform exploration mechanism\n",
    "  - decrease exploration over time\n",
    "  \n",
    "This is our first agent which is going to solve the task. It will typically require to run a few hundred of episodes to collect the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class DQNAgentWithExploration(DQNAgent):\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super(DQNAgentWithExploration, self).__init__(observation_space, action_space)\n",
    "        # exploration schedule parameters \n",
    "        self.t = 0\n",
    "        self.epsilon = ???\n",
    "        # TODO store your additional parameters here \n",
    "\n",
    "    # decay epsilon\n",
    "    def update_epsilon(self):\n",
    "        self.t += 1\n",
    "        # TODO write the code for your decay  \n",
    "        self.epsilon = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "agent = DQNAgentWithExploration(env.observation_space, env.action_space)\n",
    "env.run(agent, episodes=500, print_delay=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_7 (Dense)              (None, 30)                150       \n",
    "_________________________________________________________________\n",
    "dense_8 (Dense)              (None, 2)                 62        \n",
    "=================================================================\n",
    "Total params: 212\n",
    "Trainable params: 212\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "Episode 50, Total reward 10.0\n",
    "Episode 100, Total reward 37.0\n",
    "Episode 150, Total reward 139.0\n",
    "Episode 200, Total reward 200.0\n",
    "```\n",
    "(your mileage may vary...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "___\n",
    "\n",
    "**Your turn now !**\n",
    "\n",
    "PS: if your current DQN is really poor (not reaching reward > 10) you can cheat by using:\n",
    "```\n",
    "from decent import DQNAgent\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DQN/Explore Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### POLL: who reached a reward > 50 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### POLL: who reached a reward > 200 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Case Study\n",
    "\n",
    "### POLL: which one is DQN ? DQN/Ex ?\n",
    "\n",
    "`agent.plot_state()`\n",
    "\n",
    "| | |\n",
    "|---|--- |\n",
    "| ![](images/state_dqn_2.png) | ![](images/state_dqne_1.png) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### POLL: who got the message \"task solved\" ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### POLL: who solved in < 200 episodes ?\n",
    "\n",
    "*hint*: can we optimize gains from past experience ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DQN/Ex with Replay\n",
    "\n",
    "### Idea: correct past model updates with newest/better $Q$ function\n",
    "  \n",
    "  <img src=\"images/replay.png\" width=640>\n",
    "  <center><sub><sup>Image retrieved from http://www.modulabs.co.kr</sup></sub></center>\n",
    "  - Prioritized Replay - https://arxiv.org/abs/1511.05952 - goes one step further by weighting the sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class DQNAgentWithExplorationAndReplay(DQNAgentWithExploration):\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        ...\n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=???)\n",
    "        self.batch_size = ???\n",
    "\n",
    "    def train_model(self, action, state, next_state, reward, done):\n",
    "        \n",
    "        # save sample <s,a,r,s'> to the replay memory\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        if len(self.memory) >= self.train_start:\n",
    "            ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "___\n",
    "\n",
    "**Your turn now !**\n",
    "\n",
    "*Hint: if stuck with $\\epsilon$ in previous step you can try $\\epsilon(t) = 1 / \\sqrt{t}$*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DQN/Ex/Replay Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### POLL: who got the message \"task solved\" ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### POLL: who solved in < 200 episodes ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Case Study\n",
    "\n",
    "### POLL: Have we converged ?\n",
    "\n",
    "|  `agent.plot_state()`  |  `agent.plot_bellman()`  |\n",
    "|--|--|\n",
    "| ![](images/dqnee_state_1.png) | ![](images/dqnee_bellman_1.png)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "agent.epsilon = 0\n",
    "agent.memory = deque(maxlen=1)\n",
    "agent.batch_size = 1\n",
    "agent.train_start = 1\n",
    "env.run(agent, episodes=200, print_delay=33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### POLL: Have we converged ?\n",
    "\n",
    "`agent.plot_diagnostics()`\n",
    "\n",
    "<img src=\"images/dqnee_all_1.png\" width=640>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Potential Problems/Solution (advanced)\n",
    "\n",
    "In some settings (e.g. in some Atari games) the techniques we used are not enough.\n",
    "\n",
    "Usually inspecting Bellman residuals and exploration traces provides hints to improve.\n",
    "\n",
    "Let's see some examples...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Double DQN\n",
    "  - *assumption*: our Q estimates are too optimistic\n",
    "  - *solution*: defer model update to avoid big jumps in target\n",
    "  \n",
    "  <img src=\"images/ddqn-th.png\" width=480>\n",
    "\n",
    "  - practically: \"freeze\" $Q_2$ for several time steps / episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dueling DQN\n",
    "  - *assumption*: action is not so important for many states\n",
    "  - *solution*: separate $Q$ into state and advantage functions $Q(s,a) = V(s) + A(s,a)$\n",
    "  \n",
    "  <img src=\"images/duel.png\" width=240>\n",
    "  <center><sup><sub>Image retrieved from http://torch.ch/blog/2016/04/30/dueling_dqn.html</sub></sup></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "___\n",
    "\n",
    "You can try to implement one of these as an exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Takeaways\n",
    "\n",
    "## Basics ideas\n",
    "  - tune the DL model parameters\n",
    "  - exploration is necessary for Q-learning\n",
    "  - debug not only with the model loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " \n",
    "## Advanced ideas\n",
    "  - make exploration more efficient (replay)\n",
    "  - adapt to task specifics (DDQN, Dueling DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Next steps\n",
    "  - [John Schulman: nuts and bolts of RL research](http://rll.berkeley.edu/deeprlcourse/docs/nuts-and-bolts.pdf)\n",
    "  - try the Atari gyms... with CNNs and GPUs :)\n",
    "  - learn Policy Gradient methods"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
